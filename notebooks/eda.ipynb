{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.utils import identify_missing_quarters, preprocess_text, extract_ngrams, sentiment_polarity\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants\n",
    "YEARS = range(datetime.now().year - 5, datetime.now().year + 1)  # Last 5 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = '../data/raw/'\n",
    "transcript_files = glob.glob(os.path.join(data_directory, '*_transcripts.json'))\n",
    "\n",
    "all_transcripts = []\n",
    "\n",
    "for file in transcript_files:\n",
    "    transcript_df = pd.read_json(file)\n",
    "    all_transcripts.append(transcript_df)\n",
    "\n",
    "# Combine all DataFrames into a single DataFrame\n",
    "combined_transcripts_df = pd.concat(all_transcripts, ignore_index=True)\n",
    "\n",
    "combined_transcripts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with high level statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_transcripts_df['symbol'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_transcripts_df['symbol'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution by company\n",
    "transcript_counts = combined_transcripts_df.groupby('symbol').size()\n",
    "sns.barplot(x=transcript_counts.index, y=transcript_counts.values)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Distribution of Transcripts by Company')\n",
    "plt.ylabel('Number of Transcripts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Missing Transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Company quarters do not always line up with the calendar year. Let's start with evaluating what quarters are missing and not include the current quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_quarters_by_company = identify_missing_quarters(combined_transcripts_df)\n",
    "\n",
    "# Print out missing quarters\n",
    "for symbol, missing_quarters in missing_quarters_by_company.items():\n",
    "    if missing_quarters:\n",
    "        print(f\"{symbol} is missing transcripts for the following quarters/years: {missing_quarters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the missing quarters to make it easier to see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to represent the presence or absence of transcripts\n",
    "expected_quarters = [(q, y) for y in YEARS for q in range(1, 5)]\n",
    "heatmap_data = pd.DataFrame(index=pd.MultiIndex.from_tuples(expected_quarters, names=[\"Quarter\", \"Year\"]),\n",
    "                            columns=combined_transcripts_df['symbol'].unique())\n",
    "heatmap_data = heatmap_data.astype(float)\n",
    "\n",
    "# Fill the DataFrame: 1 for available, NaN for missing\n",
    "for symbol in combined_transcripts_df['symbol'].unique():\n",
    "    for quarter, year in expected_quarters:\n",
    "        heatmap_data.at[(quarter, year), symbol] = 1 if (quarter, year) not in missing_quarters_by_company[symbol] else np.nan\n",
    "\n",
    "quarter_year_labels = [f\"Q{q}-{y}\" for y in YEARS for q in range(1, 5)]\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Define custom color map\n",
    "cmap = sns.color_palette(\"coolwarm\", as_cmap=True)\n",
    "\n",
    "sns.heatmap(heatmap_data.T, cmap=cmap, cbar=False, \n",
    "            linewidths=.5, linecolor='grey', # adding grey cell outlines\n",
    "            xticklabels=quarter_year_labels, yticklabels=True)\n",
    "\n",
    "# Rotate labels for improved visibility\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0) \n",
    "\n",
    "# Title and labels\n",
    "plt.title('Availability of Transcripts by Quarter and Year')\n",
    "plt.xlabel('Quarter-Year')\n",
    "plt.ylabel('Ticker')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process and Inspect Transcript Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying basic processing here. Based on initial analysis and results, further processing will be needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to each transcript\n",
    "# Removing stopwords, applying lemmatization \n",
    "combined_transcripts_df['clean_content'] = combined_transcripts_df['content'].apply(preprocess_text)\n",
    "combined_transcripts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word count before cleaning\n",
    "combined_transcripts_df['word_count'] = combined_transcripts_df['content'].apply(lambda text: len(text.split()))\n",
    "\n",
    "# Word count after cleaning\n",
    "combined_transcripts_df['clean_word_count'] = combined_transcripts_df['clean_content'].apply(lambda text: len(text.split()))\n",
    "\n",
    "# Display statistics\n",
    "print(combined_transcripts_df[['word_count', 'clean_word_count']].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate word counts for each transcript\n",
    "combined_transcripts_df['word_count'] = combined_transcripts_df['content'].apply(lambda text: len(text.split()))\n",
    "combined_transcripts_df['clean_word_count'] = combined_transcripts_df['clean_content'].apply(lambda text: len(text.split()))\n",
    "\n",
    "# Plotting word counts before and after cleaning\n",
    "sns.histplot(combined_transcripts_df['word_count'], color=\"skyblue\", label='Original', kde=True)\n",
    "sns.histplot(combined_transcripts_df['clean_word_count'], color=\"red\", label='Cleaned', kde=True)\n",
    "plt.legend(title='Transcript')\n",
    "plt.xlabel('Word Count')\n",
    "plt.title('Word Counts in Transcripts Before and After Cleaning')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing separately with a little more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of word counts before cleaning\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(combined_transcripts_df['word_count'], bins=50, kde=True)\n",
    "plt.title('Histogram of Word Counts (Before Cleaning)')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Histogram of word counts after cleaning\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(combined_transcripts_df['clean_word_count'], bins=50, kde=True)\n",
    "plt.title('Histogram of Word Counts (After Cleaning)')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting Most Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all cleaned transcripts into one large string\n",
    "all_words = ' '.join(combined_transcripts_df['clean_content'])\n",
    "\n",
    "# Tokenize and count word frequencies\n",
    "# Selecting the most common 50 words\n",
    "word_freq = Counter(all_words.split())\n",
    "most_common = word_freq.most_common(50)\n",
    "print(most_common)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, the words are fairly common and do not seem very indicative of determining sentiment or gleaning insights. This was expected and I initially processed the text lightly in order to get a better idea of how the data looked closer to it's raw form. Further work and processing will be needed in order to determine better how to process here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with your most common words for the WordCloud\n",
    "word_freq_dict = {word: count for word, count in most_common}\n",
    "\n",
    "# Generate a word cloud image from the frequencies\n",
    "wordcloud = WordCloud(background_color='white', max_words=50).generate_from_frequencies(word_freq_dict)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Remove axis\n",
    "plt.title('Word Cloud for Top 50 Common Words')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sentiment polarity for each cleaned transcript\n",
    "combined_transcripts_df['sentiment'] = combined_transcripts_df['clean_content'].apply(lambda text: sentiment_polarity(text))\n",
    "\n",
    "# Plot sentiment distribution\n",
    "sns.histplot(combined_transcripts_df['sentiment'], bins=50, kde=True)\n",
    "plt.title('Sentiment Polarity Distribution')\n",
    "plt.xlabel('Sentiment Polarity')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have looked at single tokens so far, but now will take a look at bi-grams to understand what groupings of words exist and any further exploration that may be needed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = combined_transcripts_df['clean_content'].apply(lambda text: extract_ngrams(text, 2))\n",
    "\n",
    "# Counting bigrams\n",
    "bigram_freq = Counter([bigram for sublist in bigrams for bigram in sublist])\n",
    "print(bigram_freq.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be expected, word count distributions by quarter vary from company to company and quarter to quarter. Further analysis would be needed to determine how this would affect, if at all, company metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_transcripts_df['clean_word_count'] = pd.to_numeric(combined_transcripts_df['clean_word_count'], errors='coerce')\n",
    "# Create 'quarter_year' and sort\n",
    "combined_transcripts_df['quarter_year'] = combined_transcripts_df['quarter'].astype(str) + 'Q' + combined_transcripts_df['year'].astype(str)\n",
    "combined_transcripts_df.sort_values('quarter_year', inplace=True)\n",
    "\n",
    "# Create the boxplot\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.boxplot(data=combined_transcripts_df, x='quarter_year', y='clean_word_count', color='skyblue')\n",
    "plt.title('Word Count Distribution by Quarter')\n",
    "plt.xlabel('Quarter')\n",
    "plt.ylabel('Word Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average word count per ticker for each year\n",
    "average_word_counts_over_time = combined_transcripts_df.groupby(['quarter_year', 'year', 'quarter']).apply(lambda x: x['clean_word_count'].sum() / x['symbol'].nunique()).reset_index(name='average_word_count_per_ticker')\n",
    "average_word_counts_over_time.sort_values(by=['year','quarter'], inplace = True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=average_word_counts_over_time, x='quarter_year', y='average_word_count_per_ticker')\n",
    "plt.title('Industry-Level Average Word Count Trend Over Time')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Word Count per Ticker')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
